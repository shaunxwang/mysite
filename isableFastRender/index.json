[{"authors":["admin"],"categories":null,"content":"I’m a miner, interpreter and modeller of large data. A father of three lovely kids and a husband to a beautiful wife.\nI’m grateful that I have met many people in a professional career as a data wrangler. Andrew, Gina, Robert, Phil, Stephanie and many more who had pushed, nurtured and supported me along the way. A journey of five years seems long, but it’s felt like it’s just begun, there are so many things to learn in this vast and ever-expanding universe of data and computer science. The memory of exhilarating excitements are still vivid in front me when I discovered VBA in Excel and Access, and the endless possibilities of automation; relational database and SQL, then NoSQL!; open source and Linux; generalised linear model and its matrix algebra; realised 4 units maths was actually useful in deriving moments of a distribution, principle components, maximum likelihood estimators and proving law of large numbers; 80-20 rule was really referring to the area-under-the-curve of a Pareto distribution; observed normal distribution was too “normal” in practice that any population could be merely summaries by mean and (very) occasionally standard deviation, come on! But those frequentist buzz words are trivial compared to what Bayesian and iterative process have brought me – a whole wild world of non-parametric modelling: neural nets, KNN, k-means, SVM, random forest and I’m definitely not throwing buzz words at potential recruiters, frankly.\nThe latest discovery on this journey is Python. It has truly democratised my work with data. One simply can’t resist its open-sourced charm and welcoming communities but to embrace it with open arms. It’s extremely fulfilling when complex business problems are analysed very quickly and well presented/documented in a Jupyter notebook with data visualisations; it\u0026rsquo;s rewarding when my functions and modules are adopted by others in the team; and it\u0026rsquo;s satisfying when a better way of doing things is learnt with Python.\nThis universe of data science is too big to be journeyed alone, I welcome you to join me and walk it step by step, grow day by day.\n","date":1594166400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"//localhost:1313/author/shaun-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shaun-wang/","section":"authors","summary":"I’m a miner, interpreter and modeller of large data. A father of three lovely kids and a husband to a beautiful wife.\nI’m grateful that I have met many people in a professional career as a data wrangler.","tags":null,"title":"Shaun Wang","type":"authors"},{"authors":["Shaun Wang"],"categories":["Case Study"],"content":"Table of Contents  Introduction The Tools The Data The Challenge A Python solution to the challenge  Import packages Fetch data from Snowflake into a file on disk in feather format Load data of page viewed from feather file into memory and print a summary Data transformation Getting a smaller sample of pages viewed, slicing on date of visit, to be between 2019-Dec-1 and 2020-Jan-31 Getting subset of data from the sample in which number of pages viewed per each visit is between 20 and 60 Printing a summary of the final sample which we will work on Create customer journey   The Customer Journey Path Map   Introduction We are increasingly replying on digital devices for our daily lives, the convenience we gain from using them has undoubtedly revolutionised our lifestyles but at the same time data collection has also evolved with it.\nDigital footprint at every step is tracked and data is collected for analysis. There are many types of data wrangling we can do with digital tracking data:\n On the user level:  Number of visitors to a website User behavior such as click rate and bounce rate Conversions Customer journeys   On the website level  Visibility OnPage Factors Backlink analysis and monitoring   Shopping and e-commerce level:  Conversion-Tracking Tracking and monitoring in email campaigns Cross-selling campaigns   Online-advertising:  Campaign tracking Tracking affiliate links    The Tools What interests us here is customer journey, more specifically a visitor\u0026rsquo;s journey in each visit to an app or website, derived from pages viewed during the visit.\nThe tracking/tagging platform used here is not Google Analytics, that\u0026rsquo;s right, some competition in the field is always welcomed. Here, I will featured the new kid on the block, AT Internet.\nAT Internet\u0026rsquo;s tracker is initialised done via JavaScript methods which is inserted into html like so:\n\u0026lt;html\u0026gt; \u0026lt;head lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;My Page\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;smarttag.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var tag = new ATInternet.Tracker.Tag(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; And the following shows an example of a complete tagging:\nvar tag = new ATInternet.Tracker.Tag(); tag.page.send({ name:\u0026#39;pageName\u0026#39;, chapter1:\u0026#39;chap1\u0026#39;, chapter2:\u0026#39;chap2\u0026#39;, chapter3:\u0026#39;chap3\u0026#39;, level2:\u0026#39;123\u0026#39;, customObject: { param1: \u0026#39;val1\u0026#39;, param2: \u0026#39;val2\u0026#39; }, event: anEvent, callback: function(){} }); The Data There are a lot of data collected from a tag like the above, but what interests us here are the following columns:\n Visit ID of each visit Page ID of each page viewed  SDK ID of each visitor Chapter 1 of each page viewed Chapter 2 of each page viewed Chapter 3 of each page viewed D_Page a.k.a. \u0026ldquo;hit\u0026rdquo;  Usually in the format of: \u0026ldquo;chapter1::chapter2::chapter3::page_name\u0026rdquo;   Event date/time of a page being viewed Start date/time of a visit  Yes, it\u0026rsquo;s very confusing!\nYou may ask what chapter 1, chapter 2 and chapter 3 are?\nA quick look at chapter 1 for platform of website:\npg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP1\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) \\ .head(12) yields:\n   ( Top 12 ) Num. of pages     email request 13856   login 5631   registration 4819   home 4703   error 4345   cities 2539   benefits 2451   offer 2359   requests 1927   booknow 1263   concierge chat request 975   profile 795    I agree, no, it doesn\u0026rsquo;t explain anything but at least we know there is a chapter 1 called benefits.\nLet\u0026rsquo;s say that the marketing/partnership department wants to know how the benefits to our customers are doing:\npg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP1\u0026#39;]==\u0026#39;benefits\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP2\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) yields:\n   ( Top 5 ) Num. of pages     all 1751   hotel 152   dining 139   shopping 88   golf 34    We can see that chapter 2 is benefit category.\nThen, the marketing/partnership department wants to know how benefits in hotel are doing:\npg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP1\u0026#39;]==\u0026#39;benefits\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP2\u0026#39;]==\u0026#39;hotel\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP3\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) \\ .head(5) yields:\n   ( Top 5 ) Num. of pages     tokyo 4   mumbai 3   london 2   bali 1   los angeles 1    We can see that chapter 3 is the location of benefits.\nBut the the marketing/partnership department wants to know more, here is where customer journey comes in handy.\nWe can study where customer journey \u0026ldquo;forks\u0026rdquo; to after customer has browsed the benefits for the desired category and for the right location/city. Ideally, to turn revenue, we want to see customer either go to chapter 1 of booknow, email request or concierge chat request i.e. the channels of servicing. In other words, we want customers to book a request through different channels via CRM after they have browsed benefits.\nWe can then combine it with what is stored in CMS (a.k.a. content management system) about benefits to provide more granular views of customer journey within the benefits funnel.\nAt the same time, we can formulate a KPI:\n$KPI=\\frac{\\text{Number of visit in which customer has forked from benefits to channels}}{\\text{Total number of visit in which customer has been to benefits}}$\nTo recap, chapter 1 is of higher order to chapter 2 and chapter 3. What is tracked and stored in chapter 2 and chapter 3 largely depends on your digital tracking template and digital strategy.\n Here\u0026rsquo;s a very good source of digital tracking and digital strategy.\nThe Challenge Naturally, we want our customers to spend some time in the benefits funnel, i.e. browsing through a few benefits and maybe a few different cities (because there may be a positive correlation between amount of money spent and number of cities browsed, maybe). This is sometimes called depth of visit, i.e. how deep the customer has gone in each visit.\nSo the challenge for creating a customer journey based on chapter 1 is that there will be a lot of repeated values. Imaging we have a web session (part of a visit) as follows:\ngraph TD; A[Benefit A] -- B[City A]; B[City A] -- C[Benefit B]; C[Benefit B] -- D[City A]; D[City A] -- E[Benefit ALL]; E[Benefit ALL] -- F[City A]; F[City A] -- G[...]  As you can see that all of pages viewed by customer in session above have chapter 1 as benefits which will be repeated many times in the customer journey by chapter 1.\nOne may say: \u0026ldquo;easy fix mate, just take unique values of chapter 1 in each visit.\u0026rdquo;\nIt would not work because what if the customer in session above went to offer and spent some time there but then went back to benefits to browse furthermore. The second or subsequent \u0026ldquo;fork\u0026rdquo; to benefits would have been lost if we simply take the unique values of chapter 1 for each visit.\nAn example of such visit is shown below:\n   D_PAGE_CHAP1 PAGE_NUM     home 1   benefits 2   benefits 3   benefits 4   benefits 5   make a request 6   email request 7   home 8   benefits 9   benefits 10   home 11   benefits 12   benefits 13   benefits 14   make a request 15   email request 16   benefits 17   benefits 18   benefits 19   benefits 20   benefits 21   benefits 22   benefits 23   benefits 24   benefits 25   benefits 26   benefits 27   make a request 28   email request 29   benefits 30   make a request 31   email request 32   benefits 33   cities 34   cities 35   offer 36   cities 37    Customer in above visit browsed benefits six times seperately.\nA Python solution to the challenge  ⚠️ Start of a Jupyter notebook\n Import packages import pandas as pd import numpy as np Fetch data from Snowflake into a file on disk in feather format Click here for a quick guide on how to pull data from snowflake by using Python\nThe following cell are commented out because I only needed to fun it once and I have done that\n# from snowpy import run_SQL_to_feather # run_SQL_to_feather(\u0026#39;./SQL\u0026#39;,\u0026#39;data_pages.sql\u0026#39;,\u0026#39;pages.feather\u0026#39;) # run_SQL_to_feather(\u0026#39;./SQL\u0026#39;,\u0026#39;data_clicks.sql\u0026#39;,\u0026#39;clicks.feather\u0026#39;) # run_SQL_to_feather(\u0026#39;./SQL\u0026#39;,\u0026#39;data_v_contact.sql\u0026#39;,\u0026#39;v_contact.feather\u0026#39;) # print(\u0026#39;Done!\u0026#39;) pd.set_option(\u0026#39;max_colwidth\u0026#39;,None) pd.set_option(\u0026#39;min_rows\u0026#39;, 12) # pd.reset_option(\u0026#34;^display\u0026#34;) Load data of page viewed from feather file into memory and print a summary pg0 = pd.read_feather(\u0026#39;./pages.feather\u0026#39;) pg0.info(verbose=True, null_counts=True) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 6880922 entries, 0 to 6880921 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 VISITID 6880922 non-null object 1 PAGEID 6880922 non-null object 2 D_SITE 6880922 non-null category 3 PLATFORM 6880922 non-null object 4 CONTACT_UUID_JF 5411591 non-null object 5 CONTACT_UUID_SDK 6880922 non-null object 6 D_DATE_HOUR_VISIT 6880922 non-null datetime64[ns] 7 D_DATE_HOUR_EVENT 6880922 non-null datetime64[ns] 8 MIGRATION_FLAG 6880922 non-null category 9 PAGE_NAME 6880915 non-null object 10 FIRST_PAGE_NAME 6880922 non-null object 11 END_PAGE_NAME 6880901 non-null object 12 VISIT_LOGGED 6880922 non-null category 13 D_PAGE 6880922 non-null object 14 D_PAGE_CHAP1 6880913 non-null category 15 D_PAGE_CHAP2 6811048 non-null object 16 D_PAGE_CHAP3 6765013 non-null object dtypes: category(4), datetime64[ns](2), object(11) memory usage: 708.7+ MB  Data transformation The following cell are commented out because I only needed to fun it once and I have done that\n# pg0[\u0026#39;MIGRATION_FLAG\u0026#39;] = pg0[\u0026#39;MIGRATION_FLAG\u0026#39;].astype(\u0026#39;category\u0026#39;) # pg0[\u0026#39;VISIT_LOGGED\u0026#39;] = pg0[\u0026#39;VISIT_LOGGED\u0026#39;].astype(\u0026#39;category\u0026#39;) # pg0[\u0026#39;D_PAGE_CHAP1\u0026#39;] = pg0[\u0026#39;D_PAGE_CHAP1\u0026#39;].astype(\u0026#39;category\u0026#39;) # pg0.insert(pg0.columns.get_loc(\u0026#39;D_SITE\u0026#39;)+1 ,\u0026#39;PLATFORM\u0026#39; \\ # ,pg0[\u0026#39;D_SITE\u0026#39;].replace([\u0026#39;.+Android.*\u0026#39;, \u0026#39;.+[Ii]OS.*\u0026#39;, \u0026#39;.+Web.+\u0026#39;] \\ # ,[\u0026#39;Android\u0026#39;, \u0026#39;iOS\u0026#39;, \u0026#39;Web\u0026#39;] \\ # ,regex=True)) # pg0[\u0026#39;D_SITE\u0026#39;] = pg0[\u0026#39;D_SITE\u0026#39;].astype(\u0026#39;category\u0026#39;) # pg0.to_feather(\u0026#39;./pages.feather\u0026#39;) Getting a smaller sample of pages viewed, slicing on date of visit, to be between 2019-Dec-1 and 2020-Jan-31 We don\u0026rsquo;t need 7 million rows for this exercise\npg1 = \\ pg0.pipe(lambda df: df[df[\u0026#39;D_DATE_HOUR_VISIT\u0026#39;].between(\u0026#39;2019-12-01\u0026#39;,\u0026#39;2020-01-31\u0026#39;)]) \\ .reset_index(drop=True) Getting subset of data from the sample in which number of pages viewed per each visit is between 20 and 60 pg1 = pg1.pipe(lambda df: df.loc[df[\u0026#39;VISITID\u0026#39;] \\ .isin(df[\u0026#39;VISITID\u0026#39;].value_counts().to_frame(\u0026#39;PAGE_COUNT\u0026#39;) \\ .query(\u0026#34;index.str.contains(\u0026#39;JF\u0026#39;) \u0026amp; PAGE_COUNT.between(20,60)\u0026#34;) \\ .index.to_list())]) \\ .reset_index(drop=True).copy() Printing a summary of the final sample which we will work on We have about 120,000 rows\npg1.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 124344 entries, 0 to 124343 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 VISITID 124344 non-null object 1 PAGEID 124344 non-null object 2 D_SITE 124344 non-null category 3 PLATFORM 124344 non-null object 4 CONTACT_UUID_JF 76891 non-null object 5 CONTACT_UUID_SDK 124344 non-null object 6 D_DATE_HOUR_VISIT 124344 non-null datetime64[ns] 7 D_DATE_HOUR_EVENT 124344 non-null datetime64[ns] 8 MIGRATION_FLAG 124344 non-null category 9 PAGE_NAME 124342 non-null object 10 FIRST_PAGE_NAME 124344 non-null object 11 END_PAGE_NAME 124323 non-null object 12 VISIT_LOGGED 124344 non-null category 13 D_PAGE 124344 non-null object 14 D_PAGE_CHAP1 124342 non-null category 15 D_PAGE_CHAP2 108905 non-null object 16 D_PAGE_CHAP3 95794 non-null object dtypes: category(4), datetime64[ns](2), object(11) memory usage: 12.8+ MB  Producing tables for post pg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP1\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) \\ .head(12)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Num. of pages     email request 13856   login 5631   registration 4819   home 4703   error 4345   cities 2539   benefits 2451   offer 2359   requests 1927   booknow 1263   concierge chat request 975   profile 795     pg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP1\u0026#39;]==\u0026#39;benefits\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP2\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) \\ .head(5)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Num. of pages     all 1751   hotel 152   dining 139   shopping 88   golf 34     pg1 \\ .pipe(lambda df: df[df[\u0026#39;PLATFORM\u0026#39;]==\u0026#39;Web\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP1\u0026#39;]==\u0026#39;benefits\u0026#39;]) \\ .pipe(lambda df: df[df[\u0026#39;D_PAGE_CHAP2\u0026#39;]==\u0026#39;hotel\u0026#39;]) \\ .loc[:,\u0026#39;D_PAGE_CHAP3\u0026#39;] \\ .value_counts() \\ .to_frame(\u0026#39;Num. of pages\u0026#39;) \\ .head(5)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Num. of pages     tokyo 4   mumbai 3   london 2   taichung 1   bali 1     Creating a function to be pipelined(i.e. chained) with other functions, to move column to the desired loc def moving_col(df, col_name, col_name_insert_at, after=True): if isinstance(df, pd.DataFrame): loc = 0 col = df.pop(col_name) if after: loc = df.columns.get_loc(col_name_insert_at) + 1 else: loc = df.columns.get_loc(col_name_insert_at) df.insert(loc, col_name, col) return df else: return None Create customer journey 1. Create page number in each visit sorted by event date/time of page viewed ascendingly pg2 = \\ pg1[[\u0026#39;VISITID\u0026#39;,\u0026#39;PAGEID\u0026#39;,\u0026#39;D_PAGE_CHAP1\u0026#39;]] \\ .sort_values([\u0026#39;VISITID\u0026#39;,\u0026#39;PAGEID\u0026#39;]) \\ .reset_index(drop=True) \\ .assign(PAGE_NUM=lambda df: df.groupby(\u0026#39;VISITID\u0026#39;).cumcount()+1) def print_results(df): df_c = None if isinstance(df, pd.DataFrame): df_c = df.query(\u0026#39;VISITID==\u0026#34;JF - Visa APAC Android - PROD_2019-12-02_000000000000001\u0026#34;\u0026#39;) \\ .pipe(lambda df: df.loc[:,\u0026#39;D_PAGE_CHAP1\u0026#39;:]).set_index(\u0026#39;D_PAGE_CHAP1\u0026#39;,drop=True).copy() return df_c else: return df_c print_results(pg2)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PAGE_NUM   D_PAGE_CHAP1      home 1   benefits 2   benefits 3   benefits 4   benefits 5   make a request 6   email request 7   home 8   benefits 9   benefits 10   home 11   benefits 12   benefits 13   benefits 14   make a request 15   email request 16   benefits 17   benefits 18   benefits 19   benefits 20   benefits 21   benefits 22   benefits 23   benefits 24   benefits 25   benefits 26   benefits 27   make a request 28   email request 29   benefits 30   make a request 31   email request 32   benefits 33   cities 34   cities 35   offer 36   cities 37     2. Getting continuous page boolean markers: the start row of each sequence is 1, subsequent row with the same D_PAGE_CHAP1 is 0 pg2 = \\ pg2.assign(CONTINUOUS_PG_BOOL_MARKER=lambda df: \\ ((df[\u0026#39;PAGE_NUM\u0026#39;] == 1) \\ | ((df[\u0026#39;VISITID\u0026#39;] == df[\u0026#39;VISITID\u0026#39;].shift(1)) \\ \u0026amp; (df[\u0026#39;D_PAGE_CHAP1\u0026#39;] != df[\u0026#39;D_PAGE_CHAP1\u0026#39;].shift(1)))) \\ .astype(\u0026#39;int\u0026#39;)) print_results(pg2)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PAGE_NUM CONTINUOUS_PG_BOOL_MARKER   D_PAGE_CHAP1       home 1 1   benefits 2 1   benefits 3 0   benefits 4 0   benefits 5 0   make a request 6 1   email request 7 1   home 8 1   benefits 9 1   benefits 10 0   home 11 1   benefits 12 1   benefits 13 0   benefits 14 0   make a request 15 1   email request 16 1   benefits 17 1   benefits 18 0   benefits 19 0   benefits 20 0   benefits 21 0   benefits 22 0   benefits 23 0   benefits 24 0   benefits 25 0   benefits 26 0   benefits 27 0   make a request 28 1   email request 29 1   benefits 30 1   make a request 31 1   email request 32 1   benefits 33 1   cities 34 1   cities 35 0   offer 36 1   cities 37 1     3. Then, cumsum within each visit so that each member row (including the start row) of a sequence would be assigned with the same number but different to the number assigned to subsequent sequences pg2 = pg2.assign(PG_SQUNC_GROUP_MARKER=lambda df: df[\u0026#39;CONTINUOUS_PG_BOOL_MARKER\u0026#39;].cumsum()) print_results(pg2)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PAGE_NUM CONTINUOUS_PG_BOOL_MARKER PG_SQUNC_GROUP_MARKER   D_PAGE_CHAP1        home 1 1 1   benefits 2 1 2   benefits 3 0 2   benefits 4 0 2   benefits 5 0 2   make a request 6 1 3   email request 7 1 4   home 8 1 5   benefits 9 1 6   benefits 10 0 6   home 11 1 7   benefits 12 1 8   benefits 13 0 8   benefits 14 0 8   make a request 15 1 9   email request 16 1 10   benefits 17 1 11   benefits 18 0 11   benefits 19 0 11   benefits 20 0 11   benefits 21 0 11   benefits 22 0 11   benefits 23 0 11   benefits 24 0 11   benefits 25 0 11   benefits 26 0 11   benefits 27 0 11   make a request 28 1 12   email request 29 1 13   benefits 30 1 14   make a request 31 1 15   email request 32 1 16   benefits 33 1 17   cities 34 1 18   cities 35 0 18   offer 36 1 19   cities 37 1 20     4. Create rank by row within each group as marked by PG_SQUNC_GROUP_MARKER pg2 = pg2.assign(PG_SQUNC_GROUP_RANK=lambda df: \\ df.groupby([\u0026#39;VISITID\u0026#39;,\u0026#39;PG_SQUNC_GROUP_MARKER\u0026#39;])[\u0026#39;PG_SQUNC_GROUP_MARKER\u0026#39;] \\ .cumcount().astype(\u0026#39;int\u0026#39;)+1) print_results(pg2)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PAGE_NUM CONTINUOUS_PG_BOOL_MARKER PG_SQUNC_GROUP_MARKER PG_SQUNC_GROUP_RANK   D_PAGE_CHAP1         home 1 1 1 1   benefits 2 1 2 1   benefits 3 0 2 2   benefits 4 0 2 3   benefits 5 0 2 4   make a request 6 1 3 1   email request 7 1 4 1   home 8 1 5 1   benefits 9 1 6 1   benefits 10 0 6 2   home 11 1 7 1   benefits 12 1 8 1   benefits 13 0 8 2   benefits 14 0 8 3   make a request 15 1 9 1   email request 16 1 10 1   benefits 17 1 11 1   benefits 18 0 11 2   benefits 19 0 11 3   benefits 20 0 11 4   benefits 21 0 11 5   benefits 22 0 11 6   benefits 23 0 11 7   benefits 24 0 11 8   benefits 25 0 11 9   benefits 26 0 11 10   benefits 27 0 11 11   make a request 28 1 12 1   email request 29 1 13 1   benefits 30 1 14 1   make a request 31 1 15 1   email request 32 1 16 1   benefits 33 1 17 1   cities 34 1 18 1   cities 35 0 18 2   offer 36 1 19 1   cities 37 1 20 1     5. Create group size of each sequence and assign the group size to all members of the sequence pg2 = pg2.assign(PG_SQUNC_GROUP_SIZE=lambda df: \\ df.groupby([\u0026#39;VISITID\u0026#39;,\u0026#39;PG_SQUNC_GROUP_MARKER\u0026#39;])[\u0026#39;PG_SQUNC_GROUP_MARKER\u0026#39;] \\ .transform(\u0026#39;size\u0026#39;)) print_results(pg2)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PAGE_NUM CONTINUOUS_PG_BOOL_MARKER PG_SQUNC_GROUP_MARKER PG_SQUNC_GROUP_RANK PG_SQUNC_GROUP_SIZE   D_PAGE_CHAP1          home 1 1 1 1 1   benefits 2 1 2 1 4   benefits 3 0 2 2 4   benefits 4 0 2 3 4   benefits 5 0 2 4 4   make a request 6 1 3 1 1   email request 7 1 4 1 1   home 8 1 5 1 1   benefits 9 1 6 1 2   benefits 10 0 6 2 2   home 11 1 7 1 1   benefits 12 1 8 1 3   benefits 13 0 8 2 3   benefits 14 0 8 3 3   make a request 15 1 9 1 1   email request 16 1 10 1 1   benefits 17 1 11 1 11   benefits 18 0 11 2 11   benefits 19 0 11 3 11   benefits 20 0 11 4 11   benefits 21 0 11 5 11   benefits 22 0 11 6 11   benefits 23 0 11 7 11   benefits 24 0 11 8 11   benefits 25 0 11 9 11   benefits 26 0 11 10 11   benefits 27 0 11 11 11   make a request 28 1 12 1 1   email request 29 1 13 1 1   benefits 30 1 14 1 1   make a request 31 1 15 1 1   email request 32 1 16 1 1   benefits 33 1 17 1 1   cities 34 1 18 1 2   cities 35 0 18 2 2   offer 36 1 19 1 1   cities 37 1 20 1 1     6. Filtering rows to the ones which only have PG_SQUNC_GROUP_RANK as 1, PG_SQUNC_GROUP_RANK = 1 points to the first occurance of charpter 1 in a group of charpter 1\u0026rsquo;s which are the same as the the first occurance; then create charpter 1 customer journey; and create step depth for each point in charpter 1 customer journey pg_ch1_journey = \\ pg2.pipe(lambda df: df[df[\u0026#39;PG_SQUNC_GROUP_RANK\u0026#39;]==1]) \\ .assign(PG_SQUNC_GROUP_FIRST_OCCUR_NUM=lambda df: \\ df.groupby(\u0026#39;VISITID\u0026#39;).cumcount()+1) \\ .assign(CAT_STRING=lambda df: \\ \u0026#39;[\u0026#39; + df[\u0026#39;PG_SQUNC_GROUP_FIRST_OCCUR_NUM\u0026#39;].astype(\u0026#39;str\u0026#39;) + \u0026#39;] \u0026#39;) \\ .assign(TO_FORM_LIST=lambda df: \\ df[\u0026#39;CAT_STRING\u0026#39;].str.cat(df[\u0026#39;D_PAGE_CHAP1\u0026#39;])) \\ .pipe(lambda df: df.groupby(\u0026#39;VISITID\u0026#39;)[\u0026#39;TO_FORM_LIST\u0026#39;].agg(list) \\ .to_frame(\u0026#39;CHAP1_JOURNEY\u0026#39;) \\ .join(df.groupby(\u0026#39;VISITID\u0026#39;)[\u0026#39;PG_SQUNC_GROUP_SIZE\u0026#39;].agg(list) \\ .to_frame(\u0026#39;CHAP1_JOURNEY_STEP_DEPTH\u0026#39;))) pg_ch1_journey.reset_index(drop=True).head(5)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  CHAP1_JOURNEY CHAP1_JOURNEY_STEP_DEPTH     0 [[1] home, [2] benefits, [3] make a request, [4] email request, [5] home, [6] benefits, [7] home, [8] benefits, [9] make a request, [10] email request, [11] benefits, [12] make a request, [13] email request, [14] benefits, [15] make a request, [16] email request, [17] benefits, [18] cities, [19] offer, [20] cities] [1, 4, 1, 1, 1, 2, 1, 3, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 1, 1]   1 [[1] benefits, [2] contents, [3] benefits, [4] offer, [5] error, [6] offer, [7] home, [8] benefits, [9] offer, [10] error, [11] offer, [12] home] [3, 1, 1, 3, 1, 2, 1, 1, 3, 1, 2, 1]   2 [[1] home, [2] login, [3] legal, [4] help, [5] registration, [6] help, [7] home, [8] help] [1, 4, 2, 1, 2, 1, 1, 13]   3 [[1] home, [2] login, [3] legal, [4] registration, [5] legal, [6] help, [7] home, [8] help, [9] login, [10] help, [11] registration, [12] help] [1, 3, 1, 2, 1, 1, 1, 5, 1, 1, 1, 3]   4 [[1] home, [2] login, [3] home, [4] benefits, [5] home, [6] highlights, [7] offer, [8] home, [9] cities, [10] offer, [11] cities, [12] offer, [13] home, [14] requests, [15] make a request, [16] concierge chat request, [17] requests, [18] make a request, [19] requests, [20] concierge chat request, [21] requests, [22] home, [23] login, [24] home, [25] make a request, [26] concierge chat request, [27] home] [1, 1, 2, 1, 1, 1, 1, 1, 3, 2, 3, 7, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1]      ⚠️ End of a Jupyter notebook\n The Customer Journey Let\u0026rsquo;s take the following customer journey of a visist:\n [[1] benefits, [2] contents, [3] benefits, [4] offer, [5] error, [6] offer, [7] home, [8] benefits, [9] offer, [10] error, [11] offer, [12] home]\n with step depth of the following:\n [3, 1, 1, 3, 1, 2, 1, 1, 3, 1, 2, 1]\n We can quickly draw some insights:\n  Customer journey did not start with home, indicating that prior to browsing benefit customer was inactive for more than 30 minutes so a new visit was tracked when customer was active again.\n Action: check if 30 minutes threshold is appropriate. Check the proportion of those visits which were timed out. If proportion is large, deep-dive.     Customer experienced error after browsed offer and this happened two times.\n Action: check chapter 2 and chapter 3 for more details of the error. Check the proportion of forking to error after offer pages; and error happend after which chapter 3 and its proportion     Look for the customer journey of next visit by the customer to draw more insights\n  Look for the customer journey of previous visit by the customer to add that to this customer journey to form a more complete journey\n  Path Map We can draw greater insights by creating a path map based on large dataset of visits and viewed-pages:   Customer Journey Path Map   ","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"2769ec549c742e5af40049691fb3e781","permalink":"//localhost:1313/post/post_002_dig_cust_journey/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/post/post_002_dig_cust_journey/","section":"post","summary":"A pracitical example of how to create customer journey on digital platforms from digital tracking data.","tags":["Python","Pandas","Digital Tracking","Data Analysis"],"title":"Digital customer journey - a practical example","type":"post"},{"authors":null,"categories":null,"content":"","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593993600,"objectID":"6fa4e66501543c349ab57486cdca7560","permalink":"//localhost:1313/project/project_000_coming_soon/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/project/project_000_coming_soon/","section":"project","summary":"","tags":null,"title":"Coming soon...","type":"project"},{"authors":["Shaun Wang"],"categories":["Guide","Tutorial"],"content":"How to connect to Snowflake and pull data by using Python This is a quick guide on how to use my Python module to streamline your data workflow with Snowflake and Pandas, making data extraction easier.\nThe module contains two functions. One allows you to pull data from Snowflake and stores them in a csv file or pandas.DataFrame. The second function allows you to pull data from Snowflake and stores them in a feather file.\nTable of Contents  Step 1 - Install required packages Step 2 - Download the snowpy.py module Step 3 - Create a folder for SQL queries and login.txt Step 4 - How to use snowpy in Jupyter notebook Logging Python Virtual Environment   Step 1 - Install required packages pip install snowflake-connector-python[pandas] Then, save the following Python script as validate.py. Run it in terminal to check if the package is installed correctly:\nimport snowflake.connector # Gets the version ctx = snowflake.connector.connect( user=\u0026#39;\u0026lt;your_user_name\u0026gt;\u0026#39;, password=\u0026#39;\u0026lt;your_password\u0026gt;\u0026#39;, account=\u0026#39;\u0026lt;your_account_name\u0026gt;\u0026#39; ) cs = ctx.cursor() try: cs.execute(\u0026#34;SELECT current_version()\u0026#34;) one_row = cs.fetchone() print(one_row[0]) finally: cs.close() ctx.close() If you not sure about which account_name to use like how I was, you can get account_name from your Snowflake login URL.\nIf you login URL looks like the link below:\nhttps://\u0026lt;\u0026quot;company_name\u0026quot;\u0026gt;.\u0026lt;\u0026quot;region\u0026quot;\u0026gt;.snowflakecomputing.com/console/login#/ then\n\u0026lt;\u0026quot;company_name\u0026quot;\u0026gt;.\u0026lt;\u0026quot;region\u0026quot;\u0026gt; is your account_name.\nRun the Python script once you have entered user_name, password and account_name.\nSuccessful run of Python script above should look like this:\nuser@home:~$ python3 validate.py 4.22.3 It\u0026rsquo;s ok if your Snowflake version is different than mine.\nOne down one to go:\npip install snowflake-sqlalchemy Complete list of required packages can be find in the requirements.txt\nYou can also run:\npip install -r requirements.txt to save some time.\nStep 2 - Download the snowpy.py module   Download  Download snowpy.py\nor\n  Clone github repository\nor\n  Download github repository as a zip file\n  Save in folder\nSave the downoaded snowpy.py file in your working folder.\nor\nUse the downloaded repository as your working folder, it comes with a Pyhon virtual environment with all the required packages which are listed in requirement.txt.\n  If you chose to clone the repository or download it as a zip file, you should have file/folder tree looks like this:\nYour working folder | ├── login.txt ├── README.md ├── requirements.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql └── validate.py Step 3 - Create a folder for SQL queries and login.txt   SQLs folder\nCreate a folder called SQLs to store SQL queries\nYour working folder | ├── login.txt ├── README.md ├── requirements.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── 'SQLs' \u0026lt;\u0026lt;----------------------------\u0026lt; │ └── my_sql.sql └── validate.py   login.txt Create a text file called login.txt which contains three lines of text:\nyour_user_name (Snowflake) your_user_password (Snowflake) your_account_name (Snowflake) change those lines to fit your login details of Snowfake\nYour working folder | ├── 'login.txt' \u0026lt;\u0026lt;----------------------------\u0026lt; ├── README.md ├── requirements.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql └── validate.py Then set file permissions of login.txt so that root or group can not access it, i.e. only you can access and modify it.\nIf you are using a linux machine like I am, you can type the following in terminal:\nchmod go-rwx------ login.txt or\nchmod 600 login.txt Click here for detailed explanation of file permissions in linux machines\nIt\u0026rsquo;s also a good idea to add login.txt to .gitignore file so that you don\u0026rsquo;t accidentially git commit and git push login.txt with your real password to a public repository such as Github or Bitbucket.\n  Step 4 - How to use snowpy in Jupyter notebook   Install Jupyter notebook\npip install notebook   Load Jupyter notebook\nType the following in terminal\njupyter notebook Messages below will then show up in terminal, it indicates that Jupyter local server is running:\n[I 22:40:44.894 NotebookApp] Serving notebooks from local directory: /home/user/working_folder [I 22:40:44.895 NotebookApp] The Jupyter Notebook is running at: [I 22:40:44.895 NotebookApp] http://localhost:8888/?token=0cf9066b55adf4f683097cef877f696b7ad0f11234567890 [I 22:40:44.895 NotebookApp] or http://127.0.0.1:8888/?token=0cf9066b55adf4f683097cef877f696b7ad0f11234567890 [I 22:40:44.895 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:40:45.016 NotebookApp] To access the notebook, open this file in a browser: file:///home/user/.local/share/jupyter/runtime/nbserver-88888-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=0cf9066b55adf4f683097cef877f696b7ad0f11234567890 or http://127.0.0.1:8888/?token=0cf9066b55adf4f683097cef877f696b7ad0f11234567890   Create a test jupyter notebook   (A)     (B)     (C)   Your working folder | ├── login.txt ├── README.md ├── requirements.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql ├── 'test.ipynb' \u0026lt;\u0026lt;----------------------------\u0026lt; └── validate.py    (D)   See below for the test SQL query, my_sql.sql which is located in /SQLs/my_sql.sql:\nselect current_version() as CURRENT_VERSION, current_timestamp(2) as CURRENT_DATE_TIME, \u0026#39;Well Done!\u0026#39; as MESSAGE; Your working folder | ├── login.txt ├── README.md ├── requirements.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── 'my_sql.sql' \u0026lt;\u0026lt;----------------------------\u0026lt; ├── test.ipynb └── validate.py If you intend to use the second function, run_SQL_to_feather to store your data on disk in feather format, you need to install an additional package:\npip install feather-format Feather uses the Apache Arrow columnar memory specification to represent binary data on disk. This makes read and write operations very fast.\nLogging Both functions provide debug level of logging. This level of logging allows you to see what the code has done at every step.\nThe log file, snowflake_python_connector_log.txt is located here:\nYour working folder | ├── img ├── login.txt ├── __pycache__ ├── README.md ├── requirements.txt ├── 'snowflake_python_connector_log.txt' \u0026lt;\u0026lt;-----------------\u0026lt; ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql ├── test.ipynb ├── test.ipynb.md └── validate.py It\u0026rsquo;s a good idea to add snowflake_python_connector_log.txt to .gitignore file so that you don\u0026rsquo;t accidentially git commit and git push snowflake_python_connector_log.txt to a public repository like Github or Bitbucket.\nPython Virtual Environment If you chose to download or clone from Github, the files come with a Python virtual environment pre-installed, located here:\nYour working folder | ├── img ├── login.txt ├── __pycache__ ├── README.md ├── requirements.txt ├── snowflake_python_connector_log.txt ├── 'snowpy' \u0026lt;\u0026lt;----------------------------\u0026lt; │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql ├── test.ipynb ├── test.ipynb.md └── validate.py If you are using VS Code like I am, you can add the following setting to your .vscode/settings.json file, so that you will activate and use the pre-installed Python virtual environment for testing or just playing around with the two functions.\n{ \u0026#34;python.pythonPath\u0026#34;: \u0026#34;snowpy/bin/python\u0026#34; } settings.json located here:\nYour working folder | ├── .git ├── .gitignore ├── img ├── .ipynb_checkpoints ├── login.txt ├── README.md ├── requirements.txt ├── snowflake_python_connector_log.txt ├── snowpy │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ ├── pyvenv.cfg │ └── share ├── snowpy.py ├── SQLs │ └── my_sql.sql ├── test.ipynb ├── test.ipynb.md ├── validate.py └── .vscode └── 'settings.json' \u0026lt;\u0026lt;----------------------------\u0026lt;  ⚠️ The venv is built in linux machine, this means it does not have /Scripts/activate.bat or /Scripts/Activate.ps1 for you to activate venv in Windows. Sorry, Windows users need to build their own venv and install packages by using requirements.txt; or use Windows subsystem for Linux\n   ","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648000,"objectID":"178c87d2431fc984f4a159f0c00b156a","permalink":"//localhost:1313/post/post_001_snowpy/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/post/post_001_snowpy/","section":"post","summary":"This is a quick guide on how to use my Python module to streamline your data workflow with Snowflake and Pandas, making data extraction easier.","tags":["Python","Snowflake"],"title":"How to connect to Snowflake and pull data by using Python","type":"post"}]